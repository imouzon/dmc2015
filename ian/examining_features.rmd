---
title: Curation and Cross Validation to Reduce Features
titleshort: Too many features
instructions: 
output:
  usefulR::DMC_format
---

<!--- # (R code (No Results in Document)) -->
```{r set-parent, echo=FALSE,warning=FALSE,error=FALSE,message=FALSE,cache=TRUE,warning=FALSE,include=FALSE,comment=NA}
   #set up knitr
   #devtools::install_github('imouzon/usefulR')
   library(usefulR)

   #set working directory
   working.dir = "/Users/user/dmc2015/ian"
   setwd(working.dir)

   #compile the document to PDF
   if(FALSE) 
      rmarkdown::render("/Users/user/dmc2015/ian/examining_features.rmd")
```

I am using the following packages:
<!--- # (R code (No Results in Document))-->
```{r paks, echo=TRUE, cache=FALSE, message=FALSE, tidy=FALSE, include=TRUE}
   library(magrittr)
   library(dplyr)
   library(reshape2)
   library(tidyr)
   library(lubridate)
   library(ggplot2)
   library(directlabels)
   library(rCharts)
   library(xtable)
   library(foreach)
   library(gtools)
   library(knitr)
   library(utils)
   source("~/dmc2015/ian/R/renm.R")
```
My working directory is set to \verb!~/dmc2015/ian/!.

# Curating and Cross Validating

The reason features should be removed from an "large data" approach to a problem
is if they are dominated by better features.

However, when you have as many features as we do at the moment it can be difficult to 

I am starting with **set 1**

# Load feature matrix
<!--- dgg: R code (No Results in Document) -->
```{r dgg,cache=FALSE}
   ## long 
   f1 = readRDS("../data/featureMatrix/featMat_based-on-HTVset1_LONG_ver0.3.rds")

   ## wide
   d1 = readRDS("../data/featureMatrix/featMat_based-on-HTVset1_WIDE_ver0.3.rds")
```

# Check the data
<!--- chkh: R code (No Results in Document) -->
```{r chkh,cache=FALSE}
   ## isolate the X and y for set 1
   Xn = f1$train$X
   yn = f1$train$y
   
   ## remove the naive columns
   Xn = Xn[,!grepl("naive",names(Xn))]

   ## keep the validation sets
   Xv = f1$validation$X
   yv = f1$validation$y
```

And this is our loss function:
<!--- loss: R code (No Results in Document) -->
```{r loss,cache=FALSE}
   lossFunMethod = function(yval,Xval,method){
      hatmat = as.matrix(matrix(predict(method,newdata = Xval,type="response"),ncol=3,byrow=TRUE))
      ymat = matrix(yval,ncol=3,byrow=TRUE)

      error = colSums((ymat - hatmat)^2)
      wt = colMeans(ymat)^2

      cat("The coupon error is:  ",sum(error/wt),"\n")
      cat("By column error:      ",error,"\n")
      cat("By column weight:     ",wt,"\n\n")
      return(sum(error/wt))
   }
```

The following functions will help me in this process:
<!--- : R code (No Results in Document) -->
```{r cache=FALSE}
   ## I will use cross validation to estimate an error instead of checking the validation set
   RFxVal = function(CVk,Xrf,yrf,mtry=8,ntree=1000,maxnodes=50,returnERR = FALSE){
      row.order = sample(1:nrow(Xrf))
      CV.bounds = round(seq.int(1, nrow(Xrf), length=(CVk+1)))
      OOBset = lapply(1:CVk,function(i) row.order[CV.bounds[1]:(CV.bounds[2]-1)])

      #Fit a random forest for couponUsed
      cvRF = function(rows) randomForest(Xrf[-rows,], y=yrf[-rows], ntree=ntree, mtry=mtry, replace=TRUE, maxnodes = maxnodes)

      message("Fitting randomForest")
      rf_cvs = lapply(1:CVk, function(i) cvRF(OOBset[[i]]))

      message("Calculating Loss")
      lossFunction = sapply(1:CVk, function(i) lossFunMethod(yrf[OOBset[[i]]],Xrf[OOBset[[i]],],rf_cvs[[i]]))

      cat("The mean CV coupon error is:  ",mean(unlist(lossFunction)),"\n")
      if(returnERR){
         return(unlist(lossFunction))
      }else{
         return(rf_cvs)
      }

   }

   ## CV to get the mean importance
   RFxVal_imp = function(RFs){
      RFimp = data.frame("feature" = rownames(RFs[[1]]$importance), "IncNodePurity1" = RFs[[1]]$importance)
      rownames(RFimp) = NULL
      names(RFimp) = c("feature","IncNodePurity1")
      for(i in 2:length(RFs)){
         RFimp.i = data.frame("feature" = rownames(RFs[[i]]$importance), "IncNodePurity1" = RFs[[i]]$importance)
         names(RFimp.i) = c("feature",paste0("IncNodePurity",i))
         rownames(RFimp.i) = NULL
         RFimp = RFimp %>% left_join(RFimp.i,by="feature")
      }
      RFimplong = RFimp %>%
         gather(colname,IncNodePurity,-feature) %>%
         mutate(CViteration = as.numeric(gsub("IncNodePurity","",colname))) %>%
         select(feature,CViteration,IncNodePurity) %>%
         arrange(feature,CViteration)

      RFimplongMeans = RFimplong %>% 
         group_by(feature) %>% 
         summarize(meanIncNodePurity = mean(IncNodePurity))

      CVimpplotfunc = function(dsn){
         CVplot = ggplot(data=dsn, aes(x=CViteration,y=IncNodePurity, group = feature, color=feature)) +
                     geom_line() +
                     theme(legend.position = "none") + 
                     geom_text(data = RFimplong[RFimplong$CViteration == 1,],
                               aes(label = feature), hjust=1,size=.1)
         CVplot = direct.label(CVplot+xlim(0,length(RFs) + 10),"last.qp")
         return(CVplot)
      }

      RFimp = list("importance"=RFimp, 
                   "iterations" = RFimplong, 
                   "mean" =RFimplongMeans, 
                   "plot" = CVimpplotfunc(RFimplong),
                   makeplot = CVimpplotfunc)

      return(RFimp)
   }
```

#I am going to start with a simple random forest
<!--- rf: R code (No Results in Document) -->
```{r rf,cache=FALSE}
   set.seed = 1999
   ## lets try a small random forest
   library(randomForest)
   
   ## get the similarity columns
   sim_columns = c(3,grep("sim_", names(Xn)))

   ## get single way llrs:
   llr1_columns = which(grepl("llr", names(Xn)) & !grepl("X",names(Xn)))

   #The predictor and response columns
   Xrf = Xn[,c(sim_columns,llr1_columns)]

   ## I will use cross validation to estimate an error instead of checking the validation set
   set.seed = 1999
   mnodes = 5:14
   nodesCV = sapply(mnodes, function(x) RFxVal(10,Xrf,yn[,"couponUsed"],mtry=mnodes,ntree=5000,maxnodes=100,returnERR=TRUE))

   ## How many nodes to play???
   qplot(5:14,colMeans(nodesCV))

   ## looks like 11
   RFfit = RFxVal(10,Xrf,yn[,"couponUsed"],mtry=11,ntree=8000,maxnodes=100,returnERR=FALSE)

   CVrf = 


   CVrf = RFxVal(5,Xrf,yn[,"couponUsed"],mtry=4,ntree=1000,maxnodes=50,returnERR=TRUE)
   CVimp = RFxVal_imp(CVrf)

   CVimp$plot

   CVimp$makeplot(CVimp$iteration[which(CVimp$iteration$IncNodePurity > 1),])

   #fit full data
   rf1 = randomForest(Xrf, y=yn$couponUsed, ntree=1000, mtry=8, replace=TRUE, maxnodes = 50)

   #get the output
   lossFunMethod(yv$couponUsed,Xv[,which(names(Xv) %in% names(Xrf))],rf1)
```
