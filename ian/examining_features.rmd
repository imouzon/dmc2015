---
title: Curation and Cross Validation to Reduce Features
titleshort: Too many features
instructions: 
output:
  usefulR::DMC_format
---

<!--- # (R code (No Results in Document)) -->
```{r set-parent, echo=FALSE,warning=FALSE,error=FALSE,message=FALSE,cache=TRUE,warning=FALSE,include=FALSE,comment=NA}
   #set up knitr
   #devtools::install_github('imouzon/usefulR')
   library(usefulR)

   #set working directory
   working.dir = "/Users/user/dmc2015/ian"
   setwd(working.dir)

   #compile the document to PDF
   if(FALSE) 
      rmarkdown::render("/Users/user/dmc2015/ian/examining_features.rmd")
```

I am using the following packages:
<!--- # (R code (No Results in Document))-->
```{r paks, echo=TRUE, cache=FALSE, message=FALSE, tidy=FALSE, include=TRUE}
   library(magrittr)
   library(dplyr)
   library(reshape2)
   library(tidyr)
   library(lubridate)
   library(ggplot2)
   library(rCharts)
   library(xtable)
   library(foreach)
   library(gtools)
   library(knitr)
   library(utils)
   source("~/dmc2015/ian/R/renm.R")
```
My working directory is set to \verb!~/dmc2015/ian/!.

# Curating and Cross Validating

The reason features should be removed from an "large data" approach to a problem
is if they are dominated by better features.

However, when you have as many features as we do at the moment it can be difficult to 

I am starting with **set 1**

# Load feature matrix
<!--- dgg: R code (No Results in Document) -->
```{r dgg,cache=FALSE}
   ## long 
   f1 = readRDS("../data/featureMatrix/featMat_based-on-HTVset1_LONG_ver0.3.rds")

   ## wide
   d1 = readRDS("../data/featureMatrix/featMat_based-on-HTVset1_WIDE_ver0.3.rds")
```

# Check the data
<!--- chkh: R code (No Results in Document) -->
```{r chkh,cache=FALSE}
   ## isolate the X and y for set 1
   Xn = f1$train$X
   yn = f1$train$y
   
   ## remove the naive columns
   Xn = Xn[,!grepl("naive",names(Xn))]

   ## keep the validation sets
   Xv = f1$validation$X
   yv = f1$validation$y
```

And this is our loss function:
<!--- loss: R code (No Results in Document) -->
```{r loss,cache=FALSE}
   lossFunMethod = function(yval,Xval,method){
      hatmat = as.matrix(matrix(predict(method,newdata = Xval),ncol=3,byrow=TRUE))
      ymat = matrix(yval,ncol=3,byrow=TRUE)

      error = colSums((ymat - hatmat)^2)
      wt = colMeans(ymat)^2
      cat("The coupon error is:  ",sum(error/wt),"\n")
      cat("By column error:      ",error,"\n")
      cat("By column weight:     ",error,"\n\n")
      return(sum(error/wt))
   }
```

The following functions will help me in this process:
<!--- : R code (No Results in Document) -->
```{r cache=FALSE}
## I will use cross validation to estimate an error instead of checking the validation set
RFxVal = function(CVk,Xrf,yrf,mtry=8,ntree=1000,maxnodes=50){
   row.order = sample(1:nrow(Xrf))
   CVk = 40
   CV.bounds = round(seq.int(1, nrow(Xrf), length=(CVk+1)))
   OOBset = lapply(1:CVk,function(i) row.order[CV.bounds[1]:(CV.bounds[2]-1)])

   #Fit a random forest for couponUsed
   cvRF = function(rows) randomForest(Xrf[rows,], y=yrf[rows], ntree=ntree, mtry=mtry, replace=TRUE, maxnodes = maxnodes)
   rf_cvs = lapply(1:CVk, function(i) cvRF(OOBset[[i]]))
   return(rf_cvs)
}

##CV to get the mean importance
RFxVal_imp = function(RFs){
   RFimp = RFs[[1]]
   for(i in 2:length(RFs)){


```





#I am going to start with a simple random forest
<!--- rf: R code (No Results in Document) -->
```{r rf,cache=FALSE}
   set.seed = 1999
   ## lets try a small random forest
   library(randomForest)
   
   ## get the similarity columns
   sim_columns = c(3,grep("sim_", names(Xn)))

   ## get single way llrs:
   llr1_columns = which(grepl("llr", names(Xn)) & !grepl("X",names(Xn)))

   #The predictor and response columns
   Xrf = Xn[,c(sim_columns,llr1_columns)]

   ## I will use cross validation to estimate an error instead of checking the validation set
   set.seed = 1999
   row.order = sample(1:nrow(Xrf))
   CVk = 40
   CV.bounds = round(seq.int(1, nrow(Xrf), length=(CVk+1)))
   OOBset = lapply(1:CVk,function(i) row.order[CV.bounds[1]:(CV.bounds[2]-1)])

   #Fit a random forest for couponUsed
   cvRF = function(rows) randomForest(Xrf[rows,], y=yrf[rows], ntree=1000, mtry=8, replace=TRUE, maxnodes = 50)
   rf_cpn = lapply(1:CVk, function(i) cvRF(OOBset[[i]]))

   rf_cpn_imp = function(
   names(
         rf_cpn[[1]]$importance
         )
   ## print
   print(rf_cpn)

   ## plots
   varImpPlot(rf_cpn,n.var=11)
   qplot(Xrf$order_match_class,rf_cpn$predicted,geom="boxplot",fill=as.factor(f1$train$y$couponUsed))
   lossFun(rf_cpn, d1, Xv[,which(names(Xv) %in% names(Xrf))])
   #write.csv(matrix(c(Xn$orderID, rf_cpn$predicted),ncol=2),row.names=FALSE,quote=FALSE,file="./written_data/rf_prediction.csv")

   #
   lossFunMethod(yv$couponUsed,Xv[,which(names(Xv) %in% names(Xrf))],rf_cpn)

   rf_bval = randomForest(Xrf, y=f1$train$y$basketValue,  ntree=1000, mtry=10, replace=TRUE, keep.forest = TRUE, maxnodes = 100)
   print(rf_bval)
   keepercols = names(Xn[,17:20])
   dropercols = names(Xn[,c(13:16,21:30)])
```


